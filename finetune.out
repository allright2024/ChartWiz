nohup: ignoring input
[2024-07-27 21:02:25,439] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/work/ai-hub/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-07-27 21:02:28,603] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-07-27 21:02:28,604] [INFO] [runner.py:571:main] cmd = /home/work/ai-hub/anaconda3/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --lora_enable True --model_name_or_path ./checkpoints/ --version --data_path ./playground/data/llava_instruct_80k.json --image_folder /path/to/coco/train2017 --vision_tower openai/clip-vit-large-patch14 --pretrain_mm_mlp_adapter ./checkpoints/llava--pretrain/mm_projector.bin --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./checkpoints/llava--finetune_lora --num_train_epochs 1 --per_device_train_batch_size 16 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --lazy_preprocess True --dataloader_num_workers 4 --report_to wandb
[2024-07-27 21:02:31,407] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/work/ai-hub/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-07-27 21:02:34,096] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.18.5
[2024-07-27 21:02:34,097] [INFO] [launch.py:138:main] 0 NCCL_CUDA_PATH=/opt/kernel
[2024-07-27 21:02:34,097] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-07-27 21:02:34,097] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-07-27 21:02:34,097] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-07-27 21:02:34,098] [INFO] [launch.py:163:main] dist_world_size=2
[2024-07-27 21:02:34,098] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
/home/work/ai-hub/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/work/ai-hub/anaconda3/envs/llava/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[2024-07-27 21:02:38,456] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-27 21:02:38,542] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
usage: train_mem.py [-h] --version VERSION
train_mem.py: error: argument --version: expected one argument
usage: train_mem.py [-h] --version VERSION
train_mem.py: error: argument --version: expected one argument
[2024-07-27 21:02:44,110] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1585094
[2024-07-27 21:02:44,118] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1585095
[2024-07-27 21:02:44,118] [ERROR] [launch.py:321:sigkill_handler] ['/home/work/ai-hub/anaconda3/envs/llava/bin/python', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero2.json', '--lora_enable', 'True', '--model_name_or_path', './checkpoints/', '--version', '--data_path', './playground/data/llava_instruct_80k.json', '--image_folder', '/path/to/coco/train2017', '--vision_tower', 'openai/clip-vit-large-patch14', '--pretrain_mm_mlp_adapter', './checkpoints/llava--pretrain/mm_projector.bin', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './checkpoints/llava--finetune_lora', '--num_train_epochs', '1', '--per_device_train_batch_size', '16', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--lazy_preprocess', 'True', '--dataloader_num_workers', '4', '--report_to', 'wandb'] exits with return code = 2
